### 寒假总结

总的来说学习的东西挺多的，也比较杂。

刚放假的一两周时间吧，把前写过的一些ADT重写了一遍，诸如stack、queue、list之类的
已经实现过很多次了，也没什么好说的。heap的代码在我重构的时候，发现了比较多的bug，
虽然当时已经简单测试过了，但并没有发现什么问题。之后我思考了一番，heap的相关代码是
对照<<算法导论>>中的伪代码“翻译”过来的，而伪代码又是语言无关的，这样照搬过来大问题
自然是没有，但却可能会遗留一些小问题，在实际使用的时候，才会暴露出来。

在我用A\*算法重新实现snakeAI中的自动寻路算法时，由于要用到priority queue，我就想到了
之前写过的heap，打算copy过来用用，而问题就在这时候出现了，其实很简单，就是一个下标越界访问的
问题，导致出的Segment fault真是让我好找啊，可以参照一下下面的代码，其实就是忽略了对l、r的检查，因为它们可能
超过heapsize。

```c
static void
max_heap(Heap *hp, int i)
{
    l = left(i);
    r = right(i);

    assert(hp);
+   if (l > hp->heapsize)
+       return;
    if (hp->val[l] > hp->val[i])
        largest = l;
    else
        largest = i;
+   if (r > hp->heapsize)
+       goto next;
    if (hp->val[r] > hp->val[largest])
        largest = r;
next:
    if (largest != i) {
        swap(hp->val[i], hp->val[largest]);
        max_heap(hp, largest);
    }
}
```
还有就是binary search tree的删除操作，之前对算法导论中的方法不是太懂，所以就用了
"伪删除"的方法，即用待删除节点的后继节点中的数据来替换待删除节点中的数据，然后再
删除其后继节点，从而达到维护bst性质的目的。这样做的隐患就是如果其他地方保留了指向
删除节点的指针，那么再我们执行完删除操作后，它指向就不再是原来的节点了。新方法主要是
利用transplant方法来拼接两颗子树，以达到真正删除的目的。transplant方法本身很简单，
但对它的使用确实比较妙。

再有比较难的就是red-black tree的实现了，折麽了我好几天，总算让代码运行正确了。在网上
看到有人说它是2-3-4树的变形，但我不是很想去了解2-3-4🌲，以后有时间再去了解吧。

这之后呢，我就想到了我之前写过的snakeAI，虽然实现的不是很完美，仍然存在死循环的可能，但
它已经能欢快地跑好长一段时间了。snakeAI中的AI部分就是自动寻路算法的实现了，我之前使用的
是传统的BFS和DFS，但它们都是盲目式搜索算法，地图小的时候可能工作的还行，但一🥚地图
足够大的话，可能就要算死CPU了。所以我就想把它的寻路算法部分用A\*改写一下，这个想法很早就有了，
但一直没能去做。而现在刚好时间充裕，所以就想着完成它。

A\*算法呢？网上有的说很简单，有的说很难？我也是看了好多篇博客才稍微有点懂了，知道大概是怎么
操作了，但在具体编码过程中，问题其实还是挺多的。这里就不一一举例了。我写完之后，满怀欣喜的测试了
一下新版本，但让我有些失望的是没什么大的变化。这其实也没什么，可能我的地图比较小吧，体现不出来差距。A\*的
核心代码如下：

```c
    /* s(x1, y1), d(x2, y2) */
    min_pq_push(openlist, s);   /* 将s加入openlist中，它是一个优先队列 */
    open[s->x][s->y] = 1;    /* 将该点加入open中，标记为已在openlist中 */
    while ((np = min_pq_pop(openlist))) {   /* 从openlist中取出f值最小的那个点 */
        close[np->x][np->y] = 1;  /* 将取出的那个点移入close中 */
        for (int i = 0; i < 4; i++) {  
            /* 遍历它的临接点，如果不在close中也不在open中，就将它加入到openlist中 */
            int tx = np->x + dx[i];
            int ty = np->y + dy[i];
            if (!close[tx][ty] && isok(tx, ty)
                    && !is_crash_snake(tx, ty)) {
                if (!open[tx][ty]) {
                    /* 计算f值 */
                    ts->_g = np->_g + 1;
                    ts->_h = abs(tx - d->x) + abs(ty - d->y);  /* 使用曼哈顿路径 */
                    ts->_f = ts->_g + ts->_h;
                    min_pq_push(openlist, ts);
                    open[tx][ty] = 1;
                }
            }
            if (np->x == d->x && np->y == d->y)
                /* 找到了 */
        }
    }
```
然后离过年还有几天时间，看了一下<<\linux高性能服务器编程>>那本书，基本上都翻了一遍，
主要是了解了一下相关知识，代码没怎么敲，感觉有许多地方不是太懂。之后我买了陈硕的那本
<<\linux多线程服务端编程（使用muduoC++网络库）>>，年后主要是看这本书了，原因是我也想
写一个类似的网络库。

这本书上的知识还挺多，主要可分为两大部分吧。一部分是网络编程相关的C++部分，讲述了如何使用
modern C++编写程序，多线程环境下对象的管理以及RAII中一对神器shared_ptr/weak_ptr的
使用方法。然后呢就是线程同步方面的了，主张只使用mutex和condition。我觉得呢，这两个其实挺
简单好用的，读写锁我在UNP中大概看过，但也仅仅是有印象而已，自然谈不上熟悉应用了。然后呢
尽量避免使用继承与多态，使用boost::function和boost::bind，说实话这方面我不是很了解，但在muduo
库中也正如他所说的用它们来实现了相关回调。这本书我看了几遍，觉得挺有收获的，也算是知道了C++
该怎么用。我的CPPP基本没咋看，但muduo源码的主要几个类我都看了，就是用到了C with class STL以及
RAII和boost库的一点东西，确实没有用到继承和多态。之所以这样做的原因我认为是应用了太多的复杂
特性后会使整个程序晦涩难懂而且不易维护。Google的C++编程规范好像也是类似的观点。

然后就是介绍了服务端编程模型，算起来有十多种，最基础的应该是UNP最后一章描述的几种模型了，

+ 普通的迭代服务器，缺点很明显，很容易阻塞到一个用户身上而拒绝为其他用户服务。如果有一个
恶意用户，结果将不言而喻。
+ 传统的fork()模型，即每来一个client，就fork()一个子进程去处理，而父进程则继续处理新到来
的client，这样基本避免了迭代服务器的问题。但问题是进程是操作系统的稀有资源，能创建的数目是有限的，
如果有成百上千个连接，那该怎么办呢？
+ 多线程，即每个client一个thread，相比与上面的，可能线程的开销要小点。
+ 预先创建子进程，即使用进程池，这样避免了进程创建销毁的开销，只不过当server很忙的时候，还是有
一些client需要在队列中排队等一会儿。
+ 预先创建子线程，即线程池，这种模式的优点有很多，应用也比较广。使用互斥锁和条件变量就能很容易
实现一个简单的线程池。

对于进程池的实现，UNP上的做法是，主进程创建一个listenfd，然后其余子进程都accept这个listenfd，一旦
listenfd可读，睡眠在listenfd上的子进程会被随机唤醒一个，用于接受这个连接。但这其中可能会存在accept
“惊群”问题，即所有的子进程都会被唤醒，然而只能有一个子进程来接受这个新到来的连接。

这是一种简单的做法，另一种方法就是传递connfd，即主进程统一accept，然后将接受的connfd分发给其他子进程。

这些是UNP中提到的编程模型，那么结合reactor模式呢？

+ 单线程Reactor，只能在I/O线程中迭代处理用户连接。
+ 单线程Reactor + process per client / thread per client
+ 单线程Reactor + thread pool 
+ 多进程Reactor
+ 多线程Reactor
+ 多线程Reactor + thread pool 

多线程与fork()的关系

+ 在拥有超过一个控制线程的进程中fork()，它只会复制当前线程到子进程中，而其他线程都"蒸发"了。
+ 更重要的是，假如在fork之前，一个线程持有了一把🔒，而在fork之后，该进程却消失了，这也就
意味着这把🔒被永远上锁了。如果子进程中的任何一个线程lock这把锁的话，都会造成死锁。

除此之外，还有许多其他原因。因此thread与Linux传统的fork模型天生不对头。实际应用中应该避免使用。
要在多线程中使用fork，那只有fork()之后立刻exec()比较好了，如传统的看门🐶进程。

Reactor所在的线程也就是I/O线程，主要使用select/poll/epoll等I/O多路复用机制来监听多个client。
而它一般要和NIO搭配使用，因为一旦使用了一般阻塞的read/write，假如read阻塞在了某个client上，那么
整个I/O线程也将阻塞，从而拒绝为其他用户服务。这样来看，使用单线程Reactor + NIO也能实现相当程度上
的并发。但它只适合I/O密集型的应用，而不适合计算密集型的应用，道理是类似的，因为它只有一个工作线程，
不管是I/O还是计算任务都将在其中处理，一旦任务量较大，就会对处理I/O造成较大的延迟。

一个Reactor + 多线程或多进程，就是Reactor线程只负责处理I/O操作，对于每一个连接可以分配一个process
或thread给它。

一个Reactor + 线程池。这可以避免上面给每个client都创建一个thread的缺陷，如果I/O压力不大，这样的方案
会是很适合的。

多进程Reactor，这是Nginx的内置方案，如果连接之间无交互，这也是一个很好的方案。

多线程Reactor，这是muduo内置的多线程方案。即在程序开始创建固定数目的线程，然后每个线程运行一个event loop，
即one loop per thread。其中有一个Main reactor，主要负责accept连接，然后分发给其他Sub reactor。与使用
单个Reactor + 线程池相比，这样减少了进出thread pool的两次切换。并且把多个连接分散到多个Reactor线程，可以
降低相应的延迟。

最后一个，就是使用多个Reactor线程来处理I/O，使用thread pool来处理计算。既适用于有突发I/O，也适用于有
突发计算的应用。

上述琳琅满目的编程模型中，我认为比较适合初学者的是Reactor + thread pool，这个模型编程应该比较简单，
避免了复杂的线程同步问题，而且性能也不差。

上面提到了I/O多路复用一般要搭配NIO使用，而NIO则必须要有buffer才行。

我们都知道TCP是一个字符流协议，没有定义消息边界。对于简单的应用程序这或许没有什么大的麻烦，但一般而言，
应用程序都是通过message来通信的，这就需要我们自己将一定长度的字符流打包成一个message，也就是TCP分包。
这通常是encoder/decoder来做的。

那么我们来考虑这样一种情形：client A给server发送了一条消息，"hello!\n"，这条消息可能分多次到达，

    第一次："hel"
    第二次："lo"
    第三次："!\n"
假如server调用read(NIO no buffer)去读，当它读到"hel"的时候就会返回，很显然，读到的消息是不完整的。
而buffer的作用正在于此，当读到的消息不完整时，就将它放入buffer中，当读到一条完整的消息时，再通知
相关的处理程序。

服务器程序通常需要处理3种事件：I/O、信号和定时器事件。

Reactor处理timer通常是指定select/poll/epoll的超时参数，当它们返回时统一处理对应的timer和I/O事件。

组织timer通常有几种常用的数据结构：有序链表、优先队列、时间轮。

+ 有序链表是最简单的一种，每当我们添加了一个timer event后，都需要对链表进行排序，加上链表的插入效率本来就低，
所以它的效率一般，但也不能说的这么绝对，🉐️就事论事吧。
+ 优先队列，将timeout作为priority，每次以堆顶元素的timeout作为超时值，它的优点是效率高，插入删除都是O(log n)，
+ 时间轮的实现是类似于hash的思想，具有相同timeout的事件用链表串连起来。
+ muduo使用的是二叉搜索树来组织timer，采用别的方法来区分key，从而使timeout相同的事件也具有不同的key，
防止timeout相同的事件碰到一起。

在原本的单线程环境下，编写signal相关的程序就是一件麻烦事，因为signal是异步发生的。而在多线程环境下，
signal的语义更趋复杂，所以应当尽量避免处理signal。对于一般的网络编程相关的信号，我们采用它们的默认
语义即可，只处理几个不得不处理的信号，如SIGPIPE。

SIGPIPE的默认行为是终止进程。在命令行程序中，prog1 | prog2，如果prog2关闭管道的写端，而prog1继续写入的话，
就会产生SIGPIPE信号，使prog1终止。这种情况下，SIGPIPE通常是有用的。而在网络编程中，这却并不是我们
想要的，假如有一个client在连接上server之后意外终止，而此时如果server在向client继续写入的话，就会导致
服务器意外退出。同样的在服务器繁忙的时候，如果没有及时处理对方断开连接的事件，就有可能出现在连接断开后
继续发送数据的情况。

所以在启动server后，我们通常需要忽略SIGPIPE信号。(即signal(SIGPIPE, SIG_IGN))。

最后说一下我自己在写网络库过程中的感受吧，对于网络库，一开始我是挺迷惑的，主要在于不知道应该提供
怎样的接口去供用户调用，直到现在也只是大体上明白罢了。这个库我应该是重构了几次了，从最开始的不知道
如何入手，到现在的颇有成就，真可以说是感怀备至了。一开始仿照muduo写过，但写着写着就写不下去了，
仿佛被拖着鼻子走，十分难受的感觉。第二次自己写，中途仍然是放弃了，这时我才又把书和相关代码解释看了
好几遍，第三次写，才还算是颇有感觉。
